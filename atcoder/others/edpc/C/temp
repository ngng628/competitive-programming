seed = 0
class NeuralTreeRegressor(BaseEstimator, RegressorMixin):
    def __init__(self,
            n_estimators=25,
            xi=0.0,
            R=0,
            max_depth=None,
            max_features=None,
            bootstrap=True,
            random_state=None,
            verbose=0):
        self.n_estimators = n_estimators
        self.xi = xi
        self.R = R
        self.max_depth = max_depth
        self.max_features = max_features
        self.bootstrap = bootstrap
        self.random_state = random_state
        self.verbose = verbose

        if not random_state is None:
            random.seed(random_state)
            np.random.seed(random_state)


    def train(self, E):
        N = len(E)
        root = Neuron(0, *E[0])
        idx = 1
        for i in range(1, N):
            weights = E[i]
            minimumDistance = root.distance(weights[0])
            winners = list()
            winner = root
            minimumDistance, winners, winner = self.test(weights[0], root, minimumDistance, winners)

            if not (self.max_depth is None) and len(winners) >= self.max_depth:
                continue

            if minimumDistance >= self.xi:
                if not winner.children:
                    neuron = Neuron(idx, winner.weights, winner.label)
                    winner.connect(neuron)
                    idx += 1
            
                neuron = Neuron(idx, *weights)
                winner.connect(neuron)
                idx += 1
        
            for n in winners:
                n.update(weights[0], weights[1])

        return root, idx


    def test(self, weights, subRoot, minDist, winners):
        winners.append(subRoot)
        winner = subRoot
        dist = minDist
        for child in subRoot.children:
            d = child.distance(weights)
            if dist >= d:
                dist = d
                winner = child
        
        if dist < minDist:
            minDist, winners, winner = self.test(weights, winner, dist, winners)
        return minDist, winners, winner

    def fit(self, X, y, shuffle=True):       
        self.estimators = [None] * self.n_estimators
        self.n_neurons = [0] * self.n_estimators
        self.selected_dim = [None] * self.n_estimators
        X = np.array(X)
        dim = X.shape[1]
        selected_num = dim - (int(math.sqrt(dim)) if self.max_features is None else self.max_features)

        threads = []
        for i in range(self.n_estimators):
            thread = threading.Thread(target=self._fit, args=(i, X, y, dim, selected_num, shuffle))
            thread.start()
            threads.append(thread)
        
        for thread in threads:
            thread.join()

        if self.verbose != 0:
            for i, tree, in enumerate(self.estimators):
                print('====== Tree No.{} ======='.format(i+1))
                print(tree)

        return self

    def _fit(self, i, X, y, dim, selected_num, shuffle=True):
        selected = random.sample(list(range(dim)), selected_num)
        self.selected_dim[i] = selected
        ds = list(zip(np.delete(X, selected, axis=1), y))
        if shuffle: 
            np.random.shuffle(ds)

        _X = ds
        if self.bootstrap:
            _X = random.choices(ds, k=len(ds))

        tree, j = self.train(_X)
        self.estimators[i] = tree
        self.n_neurons[i] = j
        

    def test2(self, weights, subRoot):
        if subRoot.isLeaf():
            return subRoot
        winner = None
        dist = float('inf')
        for child in subRoot.children:
            d = child.distance(weights)
            if dist > d:
                dist = d
                winner = child

        return self.test2(weights, winner)


    def predict(self, X):
        X = np.array(X)
        pre_y = list()
        for e in X:
            pre_labels = np.array([], dtype=int)
            for i, subRoot in enumerate(self.estimators):
                _e = np.delete(e, self.selected_dim[i], axis=0)
                winner = self.test2(_e, subRoot)
                pre_labels = np.append(pre_labels, winner.label)
            # 重み付き平均値
            #avg = np.mean(pre_labels)
            #var = np.var(pre_labels)
            #print(DISPLAY, 'var:', var)
            #pre_y.append(sum([ y*(avg - y)**2 / var for y in pre_labels ]) / sum([(avg - y)**2 / var for y in pre_labels]))
            if self.n_estimators < 2*self.R - 1:
                if self.n_estimators % 2 == 0:
                    pre_y.append(pre_labels[self.n_estimators // 2 - 1 : self.n_estimators ].mean())
                else:
                    pre_y.append(pre_labels[self.n_estimators // 2])
            else:
                pre_y.append(pre_labels[self.R : self.n_estimators - self.R].mean())
        return pre_y


    def score(self, X, y, sample_weight=None):
        from sklearn.metrics import r2_score
        return r2_score(y, self.predict(X))

    def mse(self, X, y, sample_weight=None):
        from sklearn.metrics import mean_squared_error
        return mean_squared_error(y, self.predict(X))

    def prune(self, alpha=0.0):
        if type(alpha) is float:
            alpha = [alpha] * self.n_estimators

        for i in range(self.n_estimators):
            if self.estimators[i].children:
                self.__prune(i, self.estimators[i], alpha[i])

    def __prune(self, idx, subtree, alpha):
        for i in range(len(subtree.children)):
            if subtree.children[i].children:
                self.__prune(idx, subtree.children[i], alpha)

        if any([child.children for child in subtree.children]):
            return

        labels = [child.label for child in subtree.children]
        ave = np.average(labels)
        error = np.abs(ave - subtree.label) / subtree.label

        if error <= alpha:
            # subtree.label = label
            self.n_neurons[idx] -= len(subtree.children)
            del subtree.children[:]
print(CHECK, 'Random seed: {}'.format(seed))
n_splits = 10

dim = len(X[0])
print(DISPLAY, "dim:", dim)

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)

regressors = [NeuralTreeRegressor(n_estimators=50, xi=0.0, R=0, max_features=dim // 4),
              ]

dim = len(X_train[0])
nt = NeuralTreeRegressor(n_estimators=50, xi=0.0, R=10, max_features=dim // 2)
nt.fit(X_train, y_train)
beta = 0.0
# nt.prune(beta=beta)
print(CHECK, "SONG")

plt.plot(y_test, label='y_test')
plt.plot(nt.predict(X_test), label='SONG')

print(DISPLAY, "rmse:",np.sqrt(nt.mse(X_test, y_test)))
print(DISPLAY, "r2_score:",nt.score(X_test, y_test))

clf = RandomForestRegressor()
clf.fit(X_train, y_train)
pre = clf.predict(X_test)
# plt.plot(pre, label='Random Forest')
print(CHECK, "Random Forest")
print(DISPLAY, "rmse:", np.sqrt(metrics.mean_squared_error(y_test, pre)))
print(DISPLAY, "r2_score:", metrics.r2_score(y_test, pre))

# ラッソ
clf = Lasso(alpha=0.01)
clf.fit(X_train, y_train)
pre = clf.predict(X_test)
# plt.plot(pre, label='LASSO')
print(CHECK, "LASSO")
print(DISPLAY, "rmse:", np.sqrt(metrics.mean_squared_error(y_test, pre)))
print(DISPLAY, "r2_score:", metrics.r2_score(y_test, pre))

# catboost
# clf = CatBoost()
# clf.fit(X_train, y_train)
# pre = clf.predict(X_test)
# plt.plot(pre, label='CATBOOST')
# print(CHECK, "LASSO")
# print(DISPLAY, "rmse:", np.sqrt(metrics.mean_squared_error(y_test, pre)))
# print(DISPLAY, "r2_score:", metrics.r2_score(y_test, pre))


plt.legend()
